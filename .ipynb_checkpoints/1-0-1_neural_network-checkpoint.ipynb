{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network - 1 input, 0 hidden, 1 output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the very very simple neural network that we'll be implementing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![Network Diagram](1-0-1_nnet.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by stepping through calculation for one training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some sample data \n",
    "X = np.array([4,7,1,6,3])\n",
    "Y = np.array([0,0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init weights and biases randomly \n",
    "w1 = np.random.random()\n",
    "b1 = np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feedforward step \n",
    "i = 0 \n",
    "a0 = X[i]\n",
    "y = Y[i]\n",
    "\n",
    "# Define activation function \n",
    "def relu(x): return x if x > 0 else 0 \n",
    "\n",
    "# Calculate output \n",
    "net = a0 * w1 + b1\n",
    "a1 = relu(net) \n",
    "\n",
    "# Calculate cost \n",
    "cost = (a1 - y)**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram on the left shows the network as a multiplication graph. The diagram on the right shows the repeated applications of the chain rule to compute backpropagation.  This assumes that we are using a Relu activation function. \n",
    "![](1-0-1_nnet_backprop_combined.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropagation \n",
    "dCost_dCost = 1 \n",
    "dCost_da1 = 2*(a1 - y) * dCost_dCost\n",
    "dCost_dnet = 1 * dCost_da1 if net > 0 else 0 \n",
    "dCost_da0 = w1 * dCost_dnet   # this one is pretty useless, since we can't directly influence a0\n",
    "dCost_dw1 = a0 * dCost_dnet\n",
    "dCost_db1 = b1 * dCost_dnet\n",
    "\n",
    "# Adjusting the weights and biases \n",
    "w1 += dCost_dw1 \n",
    "b1 += dCost_db1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "That was just one example. Let's put it all together in a loop now, for all our training data, and keep running until the cost stablises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.38\n",
      "4.66\n",
      "1.48\n",
      "1.16\n",
      "1.1\n",
      "1.07\n",
      "1.05\n",
      "1.02\n",
      "1.0\n",
      "0.97\n",
      "0.95\n",
      "0.93\n",
      "0.91\n",
      "0.89\n",
      "0.87\n",
      "0.85\n",
      "0.83\n",
      "0.81\n",
      "0.8\n",
      "0.78\n",
      "0.76\n",
      "0.75\n",
      "0.73\n",
      "0.72\n",
      "0.71\n",
      "0.69\n",
      "0.68\n",
      "0.67\n",
      "0.65\n",
      "0.64\n",
      "0.63\n",
      "0.62\n",
      "0.61\n",
      "0.6\n",
      "0.59\n",
      "0.58\n",
      "0.57\n",
      "0.56\n",
      "0.55\n",
      "0.54\n",
      "0.54\n",
      "0.53\n",
      "0.52\n",
      "0.51\n",
      "0.51\n",
      "0.5\n",
      "0.49\n",
      "0.49\n",
      "0.48\n",
      "0.47\n"
     ]
    }
   ],
   "source": [
    "##### Define activation function \n",
    "def relu(x): return max(x,0)\n",
    "def sigmoid(x): return (e**x / (e**x + 1))\n",
    "\n",
    "# Create some sample data \n",
    "X = np.array([1,2,3,4,5])\n",
    "Y = np.array([0,0,1,1,1])\n",
    "\n",
    "# init weights and biases randomly \n",
    "w1 = np.random.random()\n",
    "b1 = np.random.random()\n",
    "\n",
    "# Feedforward step \n",
    "n_epochs = 5000\n",
    "activation_fun = 'relu'\n",
    "step_size = 0.0001 # how far we move with gradient descent \n",
    "for n in range(n_epochs):\n",
    "    c = 0 \n",
    "    for i in range(len(X)):\n",
    "        # determine input and output \n",
    "        a0 = X[i]\n",
    "        y = Y[i]\n",
    "\n",
    "        # Calculate output \n",
    "        net = a0 * w1 + b1\n",
    "        if   activation_fun == 'relu':      a1 = relu(net) \n",
    "        elif activation_fun == 'sigmoid':   a1 = sigmoid(net) \n",
    "            \n",
    "        # Calculate cost \n",
    "        cost = (a1 - y)**2\n",
    "\n",
    "        # Backpropagation \n",
    "        dCost_dCost = 1 \n",
    "        dCost_da1 = 2*(a1 - y) * dCost_dCost\n",
    "        if   activation_fun == 'relu':      dCost_dnet = 1 * dCost_da1 if net > 0 else 0 \n",
    "        elif activation_fun == 'sigmoid':   dCost_dnet = dCost_dnet = sigmoid(net) * ( 1 - sigmoid(net)) * dCost_da1\n",
    "        dCost_dnet = 1 * dCost_da1 if net > 0 else 0 \n",
    "        dCost_da0 = w1 * dCost_dnet   # this one is pretty useless, since we can't directly influence a0\n",
    "        dCost_dw1 = a0 * dCost_dnet\n",
    "        dCost_db1 = 1 * dCost_dnet\n",
    "\n",
    "        # Adjust the weights and biases \n",
    "        w1 += -1 * step_size * dCost_dw1 # the minus 1 is because we want to step downhill\n",
    "        b1 += -1 * step_size * dCost_db1\n",
    "\n",
    "        # Print our cost function and change in weights \n",
    "        #print(\"a0:\", a0,\"w1:\", w1, \"b1:\", b1,   \"net:\", net, \"a1:\", a1, \"y:\",y)\n",
    "        #print(dCost_da1, net, dCost_dnet, dCost_da0, dCost_dw1, dCost_db1 )\n",
    "        #print (\"cost:\", cost, \"w1Change:\", dCost_dw1, \"b1Change:\", dCost_db1 )\n",
    "        #print(cost, dCost_dw1, dCost_db1)\n",
    "        c += cost\n",
    "    if n % 100 == 0 : print(round(c,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([sigmoid(x * w1 + b1) for x in X ]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.   ,  0.069,  0.946,  1.   ,  1.   ])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
